{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "39dd021b",
      "metadata": {
        "id": "39dd021b"
      },
      "source": [
        "#  Introducción: Fine-tuning de un LLM para generación de ejercicios de inglés\n",
        "## Objetivo del notebook\n",
        "\n",
        "El objetivo de este notebook es realizar un fine-tuning de un LLM para adaptarlo a una tarea educativa concreta: la generación automática de ejercicios de inglés a partir de unas instrucciones dadas por el profesor.\n",
        "\n",
        "En concreto, el modelo aprenderá a:\n",
        "\n",
        "Recibir vocabulario específico (por ejemplo, partes de la casa, vocabulario administrativo, etc.).\n",
        "\n",
        "Tener en cuenta un tiempo verbal concreto (por ejemplo, past simple).\n",
        "\n",
        "Generar ejercicios estructurados y adaptados al aula (fill in the gaps, questions, short writing, etc.).\n",
        "## Contexto educativo\n",
        "\n",
        "Este trabajo está pensado para un contexto de Formación Profesional, donde el docente:\n",
        "\n",
        "Introduce vocabulario y gramática en clase.\n",
        "\n",
        "Necesita generar rápidamente material práctico.\n",
        "\n",
        "Quiere mantener coherencia entre ejercicios.\n",
        "\n",
        "El modelo resultante puede integrarse en:\n",
        "\n",
        "- Generación de worksheets.\n",
        "\n",
        "- Sistemas automáticos de creación de tareas.\n",
        "\n",
        "- Flujos de trabajo con herramientas como n8n o APIs de LLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "245f849d",
      "metadata": {
        "id": "245f849d"
      },
      "source": [
        "# Elección del modelo:\n",
        "\n",
        "Vamos a utilizar el modelo Mistral 7b .\n",
        "¿Por qué?\n",
        "- Es gratuito\n",
        "- Tiene buena compatibilidad con Lora y Qlora\n",
        "- Según he podido investigar, es adecuado para un contexto de aprendizaje\n",
        "\n",
        "Principales características:\n",
        "- Aproximadamente 7.000 millones de parámetros.\n",
        "- Es un modelo openweight, que, según vimos en clase, nos deja sus pesos abi\n",
        "- Entrenado específicamente para seguir instrucciones.\n",
        "- Excelente calidad de generación para su tamaño.\n",
        "- Arquitectura moderna y eficiente.\n",
        "- Totalmente compatible con LoRA y QLoRA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a4a2b6f",
      "metadata": {
        "id": "8a4a2b6f"
      },
      "source": [
        "# Instalación de librerías y comprobación de entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "EWYba_AbMpf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWYba_AbMpf6",
        "outputId": "3b6eee00-7a3b-4349-e241-9f605db9c1e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.5.1+cu121\n",
            "Uninstalling torch-2.5.1+cu121:\n",
            "  Successfully uninstalled torch-2.5.1+cu121\n",
            "Found existing installation: torchvision 0.20.1+cu121\n",
            "Uninstalling torchvision-0.20.1+cu121:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu121\n",
            "Found existing installation: torchaudio 2.5.1+cu121\n",
            "Uninstalling torchaudio-2.5.1+cu121:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
            "Found existing installation: bitsandbytes 0.42.0\n",
            "Uninstalling bitsandbytes-0.42.0:\n",
            "  Successfully uninstalled bitsandbytes-0.42.0\n"
          ]
        }
      ],
      "source": [
        "pip uninstall -y torch torchvision torchaudio bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "CSJQuxqPMspY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSJQuxqPMspY",
        "outputId": "2331a5de-9afe-4732-f193-17e0158feafa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.2 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7jNaj0GaPCSS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jNaj0GaPCSS",
        "outputId": "15ec4bf3-9e55-4b20-d440-56177286fdf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1+cu121\n",
            "12.1\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AqGg9ZdRPCGg",
      "metadata": {
        "id": "AqGg9ZdRPCGg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e0ebdb90",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0ebdb90",
        "outputId": "f129d629-5edf-465e-b919-0bb95bf5a870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA disponible: True\n",
            "Dispositivo: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA disponible:\", torch.cuda.is_available())\n",
        "print(\"Dispositivo:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e44e6112",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e44e6112",
        "outputId": "a0414bc0-c9e9-4814-ec3b-b4f364943c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: transformers 4.38.2\n",
            "Uninstalling transformers-4.38.2:\n",
            "  Successfully uninstalled transformers-4.38.2\n",
            "Found existing installation: tokenizers 0.15.2\n",
            "Uninstalling tokenizers-0.15.2:\n",
            "  Successfully uninstalled tokenizers-0.15.2\n",
            "Found existing installation: accelerate 0.27.2\n",
            "Uninstalling accelerate-0.27.2:\n",
            "  Successfully uninstalled accelerate-0.27.2\n",
            "Found existing installation: peft 0.9.0\n",
            "Uninstalling peft-0.9.0:\n",
            "  Successfully uninstalled peft-0.9.0\n",
            "Found existing installation: trl 0.7.11\n",
            "Uninstalling trl-0.7.11:\n",
            "  Successfully uninstalled trl-0.7.11\n",
            "Collecting transformers==4.38.2\n",
            "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "Collecting tokenizers==0.15.2\n",
            "  Using cached tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting accelerate==0.27.2\n",
            "  Using cached accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting peft==0.9.0\n",
            "  Using cached peft-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl==0.7.11\n",
            "  Using cached trl-0.7.11-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.27.2) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.27.2) (2.5.1+cu121)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from trl==0.7.11) (4.0.0)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.12/dist-packages (from trl==0.7.11) (1.0.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (1.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.27.2) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.27.2) (1.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.7.11) (0.17.0)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro>=0.5.11->trl==0.7.11) (4.4.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.7.11) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.7.11) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.7.11) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.7.11) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->trl==0.7.11) (0.70.16)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.38.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.38.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.38.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.38.2) (2026.1.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.7.11) (3.13.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.7.11) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.7.11) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->trl==0.7.11) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.7.11) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.7.11) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.7.11) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.7.11) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.7.11) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.7.11) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.7.11) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.7.11) (1.17.0)\n",
            "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "Using cached tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Using cached accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "Using cached peft-0.9.0-py3-none-any.whl (190 kB)\n",
            "Using cached trl-0.7.11-py3-none-any.whl (155 kB)\n",
            "Installing collected packages: tokenizers, transformers, accelerate, trl, peft\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 5.2.2 requires transformers<6.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.27.2 peft-0.9.0 tokenizers-0.15.2 transformers-4.38.2 trl-0.7.11\n"
          ]
        }
      ],
      "source": [
        "%pip uninstall -y transformers tokenizers accelerate peft trl\n",
        "%pip install \\\n",
        "  transformers==4.38.2 \\\n",
        "  tokenizers==0.15.2 \\\n",
        "  accelerate==0.27.2 \\\n",
        "  peft==0.9.0 \\\n",
        "  trl==0.7.11\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "05GM9zGqY86C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05GM9zGqY86C",
        "outputId": "4a96f235-5fd9-4c63-e558-d08c4fed212c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes==0.42.0\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.42.0) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->bitsandbytes==0.42.0) (2.0.2)\n",
            "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.42.0\n"
          ]
        }
      ],
      "source": [
        "pip install bitsandbytes==0.42.0 --no-cache-dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48Q_qlAx8r7X",
      "metadata": {
        "id": "48Q_qlAx8r7X"
      },
      "source": [
        "# Carga del modelo base Mistral 7B instruct:\n",
        "Para esto, usaremos además una cuantización a 4 bits. De esa manera, reduciremos el uso de memoria GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fiiynDOM82Vw",
      "metadata": {
        "id": "fiiynDOM82Vw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "By3hhcYT864l",
      "metadata": {
        "id": "By3hhcYT864l"
      },
      "outputs": [],
      "source": [
        "# Para realizar la cuantización a 4 bits\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "361fd54b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "361fd54b",
        "outputId": "6eaf5d01-1deb-4e68-dcb3-48ad8ac8ba90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bitsandbytes is installed.\n",
            "bitsandbytes version: 0.42.0\n",
            "accelerate is installed.\n",
            "accelerate version: 0.27.2\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import bitsandbytes\n",
        "    print(\"bitsandbytes is installed.\")\n",
        "    print(f\"bitsandbytes version: {bitsandbytes.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"bitsandbytes is NOT installed.\")\n",
        "\n",
        "try:\n",
        "    import accelerate\n",
        "    print(\"accelerate is installed.\")\n",
        "    print(f\"accelerate version: {accelerate.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"accelerate is NOT installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "45292b3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45292b3d",
        "outputId": "80076f62-d169-47b9-c193-7457075cce45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "++++++++++++++++++ BUG REPORT INFORMATION ++++++++++++++++++\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "++++++++++++++++++ /usr/local CUDA PATHS +++++++++++++++++++\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda120_nocublaslt.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda114.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda115.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda111.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda120.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda111_nocublaslt.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda123.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda110.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda115_nocublaslt.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda122_nocublaslt.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda123_nocublaslt.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda110_nocublaslt.so\n",
            "/usr/local/lib/python3.12/dist-packages/bitsandbytes/libbitsandbytes_cuda114_nocublaslt.so\n",
            "/usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda.so\n",
            "/usr/local/lib/python3.12/dist-packages/torch/lib/libc10_cuda.so\n",
            "/usr/local/lib/python3.12/dist-packages/torch/lib/libtorch_cuda_linalg.so\n",
            "/usr/local/lib/python3.12/dist-packages/jax_cuda12_plugin/cuda_plugin_extension.so\n",
            "/usr/local/lib/python3.12/dist-packages/libucx/lib/ucx/libucx_perftest_cuda.so\n",
            "/usr/local/lib/python3.12/dist-packages/libucx/lib/ucx/libucm_cuda.so\n",
            "/usr/local/lib/python3.12/dist-packages/libucx/lib/ucx/libuct_cuda.so\n",
            "/usr/local/lib/python3.12/dist-packages/jax_plugins/xla_cuda12/xla_cuda_plugin.so\n",
            "/usr/local/lib/python3.12/dist-packages/pylibraft/common/cuda.cpython-312-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.12/dist-packages/rmm/pylibrmm/cuda_stream.cpython-312-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.12/dist-packages/cuda/core/experimental/_utils/cuda_utils.cpython-312-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.12/dist-packages/cuda/ccuda.cpython-312-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.12/dist-packages/cuda/cudart.cpython-312-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.12/dist-packages/cuda/ccudart.cpython-312-x86_64-linux-gnu.so\n",
            "/usr/local/lib/python3.12/dist-packages/cuda/cuda.cpython-312-x86_64-linux-gnu.so\n",
            "/usr/local/cuda-12.5/targets/x86_64-linux/lib/libcudart.so\n",
            "/usr/local/cuda-12.5/targets/x86_64-linux/lib/stubs/libcuda.so\n",
            "/usr/local/cuda-12.5/compat/libcuda.so\n",
            "\n",
            "+++++++++++++++ WORKING DIRECTORY CUDA PATHS +++++++++++++++\n",
            "\n",
            "\n",
            "++++++++++++++++++ LD_LIBRARY CUDA PATHS +++++++++++++++++++\n",
            "+++++++++++++++ /usr/lib64-nvidia CUDA PATHS +++++++++++++++\n",
            "/usr/lib64-nvidia/libcuda.so\n",
            "\n",
            "++++++++++++++++++++++++++ OTHER +++++++++++++++++++++++++++\n",
            "COMPILED_WITH_CUDA = True\n",
            "COMPUTE_CAPABILITIES_PER_GPU = ['7.5']\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "++++++++++++++++++++++ DEBUG INFO END ++++++++++++++++++++++\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "\n",
            "Running a quick check that:\n",
            "    + library is importable\n",
            "    + CUDA function is callable\n",
            "\n",
            "\n",
            "WARNING: Please be sure to sanitize sensible info from any such env vars!\n",
            "\n",
            "SUCCESS!\n",
            "Installation was successful!\n"
          ]
        }
      ],
      "source": [
        "!python -m bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4fIPEhMV9CFB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fIPEhMV9CFB",
        "outputId": "c093caac-ba26-48d0-e0d8-ac9236b489c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Carga del tokenizador\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_fast=False\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "zA5OmMpt9avC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "656d0cbe27b54166a7b19b84c2dd909d",
            "532c2bc7f6d54a6280a7d89760e90cbe",
            "f723d01802104a5daeabb1a29cf8a705",
            "9782e0af68934df984308dd9cf4b87a6",
            "164e44f7c7464e718507925e26ea898f",
            "d285bd18989c494fa31de83cabb93ef2",
            "9c2d4477aa3e484aa2a1983ebfbbc494",
            "be8336e53ff64b6b99165323c0ae4e38",
            "62d262e494124811b57797677acba505",
            "2506de45fe3648d0a41d4d92d62c418a",
            "239f2a5afece45bbb125145388bec625"
          ]
        },
        "id": "zA5OmMpt9avC",
        "outputId": "2e5a41cb-b968-411a-dc22-0ef0883929c6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "656d0cbe27b54166a7b19b84c2dd909d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Carga del modelo cuantizado\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "TiBBbfQV9gKs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiBBbfQV9gKs",
        "outputId": "6df6a891-ec0e-4969-958b-5edb1606d9f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo cargado en: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Comprobación del dispositivo en el que está trabajando el modelo\n",
        "print(\"Modelo cargado en:\", model.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "MjfVB7UH9neY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjfVB7UH9neY",
        "outputId": "361ac249-e982-4686-e645-d77a6aae55ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create a short English exercise using the past simple tense.\n",
            "\n",
            "Title: A Day in the Life of a Student\n",
            "\n",
            "Instructions:\n",
            "\n",
            "Read the text below and answer the questions that follow.\n",
            "\n",
            "Text:\n",
            "\n",
            "Yesterday, I went to the university. I took the bus at 8:30 a.m. and arrived at the campus at 9:15. My first class started at 9:30 and it was English. We learned new vocabulary words and practiced speaking in groups. After\n"
          ]
        }
      ],
      "source": [
        "#prueba del modelo:\n",
        "prompt = \"Create a short English exercise using the past simple tense.\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0UADeFWkNu-p",
      "metadata": {
        "id": "0UADeFWkNu-p"
      },
      "source": [
        "Una vez que tenemos esto hecho, vamos a dar el siguiente paso, que será, cargar el dataset de ejemplos de tareas en un json. Este dataset es sintético, ha sido extraído de chatgpt tras haberle dado algunas directrices del tipo de tareas que debería crear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "sUn2dWW8Nsf1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUn2dWW8Nsf1",
        "outputId": "31baf5b4-f76c-4932-937c-9a5407ceb04d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['messages'],\n",
              "    num_rows: 60\n",
              "})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importamos la función para cargar datasets desde Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Cargamos el dataset desde un archivo JSONL.\n",
        "# Cada línea del archivo corresponde a un ejemplo de entrenamiento.\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"dataset.jsonl\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "# Mostramos un resumen del dataset para verificar que se ha cargado correctamente\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "XN923yoAOPLg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN923yoAOPLg",
        "outputId": "d17f1c33-5c23-45e3-f2d9-c1dd787bd927"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [{'role': 'system',\n",
              "   'content': 'You are an English teacher for vocational training students. You create vocabulary exercises adapted to A2 level.'},\n",
              "  {'role': 'user',\n",
              "   'content': 'Create a vocabulary exercise about office objects.'},\n",
              "  {'role': 'assistant',\n",
              "   'content': 'Exercise 1: Match the words with the correct object.\\n\\n1. Printer\\n2. Desk\\n3. Chair\\n4. Computer\\n\\nA. Used to sit\\nB. Used to print documents\\nC. Used to work with files\\nD. Used to place papers'}]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#comprobamos el dataset\n",
        "dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "TWZH3NTBNuOL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWZH3NTBNuOL",
        "outputId": "f4c73cc5-b564-40d1-c9bd-c3c355f81ac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'messages': List({'role': Value('string'), 'content': Value('string')})}\n",
            "60\n"
          ]
        }
      ],
      "source": [
        "# Mostramos las columnas (features) del dataset para comprobar su estructura\n",
        "print(dataset.features)\n",
        "\n",
        "# Mostramos el número total de ejemplos de entrenamiento\n",
        "print(dataset.num_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "RqR3yl72Ok5H",
      "metadata": {
        "id": "RqR3yl72Ok5H"
      },
      "outputs": [],
      "source": [
        "def format_prompt(example):\n",
        "    # Convertimos la conversación (messages) en un único texto de entrenamiento\n",
        "    prompt = \"\"\n",
        "\n",
        "    for message in example[\"messages\"]:\n",
        "        role = message[\"role\"].upper()\n",
        "        content = message[\"content\"]\n",
        "        prompt += f\"[{role}]\\n{content}\\n\\n\"\n",
        "\n",
        "    return {\"text\": prompt.strip()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "Wrj22LfBOyQL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wrj22LfBOyQL",
        "outputId": "30ca36a2-272b-4741-8a6e-e676492abfb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SYSTEM]\n",
            "You are an English teacher for vocational training students. You create vocabulary exercises adapted to A2 level.\n",
            "\n",
            "[USER]\n",
            "Create a vocabulary exercise about office objects.\n",
            "\n",
            "[ASSISTANT]\n",
            "Exercise 1: Match the words with the correct object.\n",
            "\n",
            "1. Printer\n",
            "2. Desk\n",
            "3. Chair\n",
            "4. Computer\n",
            "\n",
            "A. Used to sit\n",
            "B. Used to print documents\n",
            "C. Used to work with files\n",
            "D. Used to place papers\n"
          ]
        }
      ],
      "source": [
        "dataset = dataset.map(format_prompt)\n",
        "print(dataset[0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "kOTiQSU3Pv-t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "173f42cc80a84d14843834c43e8ae617",
            "6ed38659da254ea595bf37dd5847a33b",
            "4bddf8fccddf45f991220d8e14f94031",
            "4ed5c22356b74ae88c2f95d10d5c83f9",
            "f5767c46597e42a18793c9e61e01081d",
            "836a07a301524c709a9e5b547c7396e9",
            "f2c3004756dc423289f89f002336cbd6",
            "77bed94f35b14359bf5cf747cea56351",
            "50dabfb5338546b991340ee04b96e59b",
            "21ccbc61e59d40cfbcbb911db51e5869",
            "2591d0bf349a46ee8544c2774651f4b7"
          ]
        },
        "id": "kOTiQSU3Pv-t",
        "outputId": "6a51631c-0371-4b85-d731-0e1fccb1bc00"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "173f42cc80a84d14843834c43e8ae617",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Tokenización del dataset\n",
        "def tokenize(example):\n",
        "    tokens = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=False\n",
        "    )\n",
        "# Añadimos labels al dataset para un correcto entrenamiento\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "\n",
        "    return tokens\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize,\n",
        "    remove_columns=dataset.column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z-RL2Yx6ZgqQ",
      "metadata": {
        "id": "z-RL2Yx6ZgqQ"
      },
      "source": [
        "Una vez hemos preparado el modelo y el dataset para nuestra tarea, podemos empezar a entrenar el proceso en 4- bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "HlPwL1UvQSOR",
      "metadata": {
        "id": "HlPwL1UvQSOR"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2e7xCZJ9QVoE",
      "metadata": {
        "id": "2e7xCZJ9QVoE"
      },
      "outputs": [],
      "source": [
        "#Configuración de LoRA\n",
        "#Definimos los parámetros que controlan qué partes del modelo se entrenan\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.05, #esta capa evita el sobreajuste durante el entrenamiento\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "RoLuRax1Qa1g",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoLuRax1Qa1g",
        "outputId": "7b1cac36-7c8c-4398-c3bb-462004539737"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralSdpaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm()\n",
              "            (post_attention_layernorm): MistralRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# aplicamos lora al modelo:\n",
        "model = get_peft_model(model, lora_config)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6mKVQez-Qi2V",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mKVQez-Qi2V",
        "outputId": "0c0a24ba-3a04-4aaa-8d92-aafae06fb25e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 41,943,040 || all params: 7,283,675,136 || trainable%: 0.5758499550960753\n"
          ]
        }
      ],
      "source": [
        "#Comprobamos el número de parámetros a entrenar.\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "NGANZ2opbaPR",
      "metadata": {
        "id": "NGANZ2opbaPR"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "# Definimos los parámetros del entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",            # Carpeta donde se guardan los resultados\n",
        "    num_train_epochs=3,                # Número de épocas de entrenamiento\n",
        "    per_device_train_batch_size=1,     # Batch size por GPU\n",
        "    gradient_accumulation_steps=4,     # Simula un batch mayor acumulando gradientes\n",
        "    learning_rate=2e-4,                # Learning rate típico para LoRA\n",
        "    fp16=True,                         # Entrenamiento en media precisión\n",
        "    logging_steps=10,                  # Frecuencia de logging\n",
        "    save_strategy=\"epoch\",             # Guardar el modelo al final de cada época\n",
        "    report_to=\"none\"                   # Desactivamos servicios externos de logging\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3GzlFgLabk8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GzlFgLabk8f",
        "outputId": "54da3424-d023-44d1-f003-0a1fc7930857"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Inicializamos el entrenador para fine-tuning supervisado\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "LHcOU0mNbkwT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "LHcOU0mNbkwT",
        "outputId": "2025405b-e219-4b6a-9460-c61cd2070413"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [45/45 02:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.721700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.609700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.369600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.174600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory ./results/checkpoint-15 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Checkpoint destination directory ./results/checkpoint-30 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Checkpoint destination directory ./results/checkpoint-45 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=45, training_loss=0.6585872809092204, metrics={'train_runtime': 172.9147, 'train_samples_per_second': 1.041, 'train_steps_per_second': 0.26, 'total_flos': 642961801101312.0, 'train_loss': 0.6585872809092204, 'epoch': 3.0})"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Comenzamos el proceso de entrenamiento\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1Q9-GXmdkHm8",
      "metadata": {
        "id": "1Q9-GXmdkHm8"
      },
      "source": [
        "Con nuestro modelo ya entrenado, vamos a hacer pruebas de éste para ver cómo funciona en distintos ejercicios. Pese a que aquí incluyo una sola versión, este prompt ha pasado por varias etapas de refinamiento hasta encontrar un resultado satifactorio\n",
        "\n",
        "Vamos a probar los siguientes ejercicios:\n",
        "-Word formation\n",
        "-Reading comprehension + preguntas\n",
        "-Grammar (Fill in the gaps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "p0nc0wl7dRx_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0nc0wl7dRx_",
        "outputId": "dc6b82d8-c4e8-4364-a479-9b75a9d35449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a list of words separated by commas: describe, king, science, apology, protect, warm, art, die, true, empire, locate, select, fascinate, luck, poem, mystery, succeed, destroy, honest, high, consider, hope, meaning, appear, nation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ASSISTANT]\n",
            "Exercise:\n",
            "\n",
            "1. She gave a clear ______ of the problem. (DESCRIBE)\n",
            "2. He works in the field of ______. (SCIENCE)\n",
            "3. The documents were under company ______. (PROTECT)\n",
            "4. His sudden ______ shocked everyone. (APPEAR)\n",
            "5. We discussed the real ______ of the text. (MEANING)\n",
            "\n",
            "Answers:\n",
            "1. description\n",
            "2. science\n",
            "3. protection\n",
            "4. appearance\n",
            "5. meaning\n",
            "\n",
            "Prompts:\n",
            "1. describe\n",
            "2. scientific\n",
            "3. protect\n",
            "4. appear\n",
            "5. mean\n",
            "\n",
            "Explanation:\n",
            "The exercise requires students to change words by adding prefixes or suffixes. The prompts give the base form for the answers. For example, the prompt \"describe\" suggests the answer \"description\". All sentences imply that the base word must be changed by the student.\n"
          ]
        }
      ],
      "source": [
        "# Ejercicio word- formation\n",
        "# Lista de vocabulario a probar: describe, king, science, apology, protect, warm, art, die, true, empire, locate, select, fascinate, luck, poem, mystery, succeed, destroy, honest, high, consider, hope, meaning, appear, nation\n",
        "\n",
        "# =========================\n",
        "# INPUT DEL USUARIO\n",
        "# =========================\n",
        "\n",
        "# El usuario introduce las palabras separadas por comas\n",
        "vocabulary_input = input(\"Enter a list of words separated by commas: \")\n",
        "templates = [\n",
        "    \"She gave a clear ___ of the problem.\",\n",
        "    \"He works in the field of ___.\",\n",
        "    \"The documents were under company ___.\",\n",
        "    \"His sudden ___ shocked everyone.\",\n",
        "    \"We discussed the real ___ of the text.\",\n",
        "    \"She became a famous ___.\",\n",
        "    \"The final ___ was announced yesterday.\",\n",
        "    \"With a bit of ___, everything is possible.\"\n",
        "]\n",
        "# Convertimos el texto en una lista de palabras limpia\n",
        "vocabulary = [word.strip() for word in vocabulary_input.split(\",\")]\n",
        "# =========================\n",
        "# PROMPT COMPLETO\n",
        "# =========================\n",
        "prompt = f\"\"\"[SYSTEM]\n",
        "You are an English teacher creating B1 word formation exercises.\n",
        "\n",
        "[USER]\n",
        "Use ONLY the sentence templates below to create a word formation exercise similar to this one.\n",
        "\n",
        "Sentence templates:\n",
        "1. She gave a clear ______ of the problem. (DESCRIBE)\n",
        "Answer: description\n",
        "\n",
        "2. He works in the field of ______. (SCIENCE)\n",
        "Answer: science\n",
        "\n",
        "3. The documents were under company ______. (PROTECT)\n",
        "Answer: protection\n",
        "\n",
        "4. His sudden ______ shocked everyone. (APPEAR)\n",
        "Answer: appearance\n",
        "\n",
        "5. We discussed the real ______ of the text. (MEANING)\n",
        "Answer: meaning\n",
        "\n",
        "\n",
        "\n",
        "For each sentence:\n",
        "-  Create the sentence from scratch\n",
        "- Give a prompt (a word in brackets) that serves the student as a base to create the answer to the exercise\n",
        "- Make sure the exercise requires the student to write a word with preffixes or suffixes\n",
        "- Add the base word in CAPITAL LETTERS in brackets at the end of each sentence\n",
        "- Check a second time that all the sentences imply that the base word or prompt must be changed by the student\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# =========================\n",
        "# INFERENCIA CONTROLADA\n",
        "# =========================\n",
        "# Tokenizamos el prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "prompt_length = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=250,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.1,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# Nos quedamos SOLO con los nuevos tokens (la respuesta)\n",
        "generated_tokens = outputs[0][prompt_length:]\n",
        "\n",
        "result = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rHm19l27z7wA",
      "metadata": {
        "id": "rHm19l27z7wA"
      },
      "source": [
        "# Evaluación ejercicio Word formation:\n",
        "Tras refinar el prompt, he comprobado que estos ejercicios no son fáciles de conseguir de manera útil. En este caso, no tiene sentido informar al prompt del tiempo verbal en el que deberían contextualizar el ejercicio, dado que el LLM entiende que debe hacer un ejercicio basado en tiempos verbales y gramática.\n",
        "\n",
        "Exercise:\n",
        "\n",
        "1. She gave a clear ______ of the problem. (DESCRIBE)\n",
        "2. He works in the field of ______. (SCIENCE)\n",
        "3. The documents were under company ______. (PROTECT)\n",
        "4. His sudden ______ shocked everyone. (APPEAR)\n",
        "5. We discussed the real ______ of the text. (MEANING)\n",
        "\n",
        "Answers:\n",
        "1. description\n",
        "2. science\n",
        "3. protection\n",
        "4. appearance\n",
        "5. meaning\n",
        "\n",
        "Prompts:\n",
        "1. describe\n",
        "2. scientific\n",
        "3. protect\n",
        "4. appear\n",
        "5. mean\n",
        "\n",
        "Como vemos, en este caso ha creado distntas palabras, todas ellas para ser cambiadas a sustantivo, o nominalizadas.\n",
        "Este resultado denota que el modelo encuentra dificultades en la variedad de transformaciones (no crea adverbios, adjetivos, verbos...)\n",
        "\n",
        "Si bien estos ejercicios son aceptables como un primer acercamiento, no nos da una variedad satisfactoria como para pensar que este caso de uso ha sido útil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "1eQorMF7hOe4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eQorMF7hOe4",
        "outputId": "f783d4c2-1460-460d-e3b7-1a903a399d23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Include the grammar focus (e.g. past simple, modal verbs, conditionals): conditionals\n",
            "Enter a list of vocabulary words separated by commas: hiking, camping, rock climbing, cycling, kayaking, canoeing, surfing, paddle boarding, skiing, snowboarding, trail running, mountain biking, nature walk, outdoor workout, team sports, adventure sports, physical activity, fresh air, natural environment, open spaces, equipment, safety rules, protective gear, helmet, comfortable clothing, weather conditions, sunny day, rainy weather, cold temperatures, warm temperatures, risk, challenge, endurance, strength, balance, coordination, teamwork, motivation, mental health, physical health, stress reduction, well-being, healthy lifestyle, free time, leisure activities, outdoor experience\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading Text:\n",
            "Yesterday, Mark went hiking with his friends. They prepared their backpacks and checked the safety rules before starting the trail. Mark enjoyed the beautiful views and the fresh air. After the hike, they had lunch at a campsite and did some rock climbing. In the afternoon, they went cycling and kayaking. Mark felt energized and motivated. He was grateful for the opportunity to spend time in nature and disconnect from work.\n",
            "\n",
            "Comprehension Questions:\n",
            "1. Why did Mark go hiking?\n",
            "2. What activities did he do with his friends?\n",
            "3. How did he feel after the day?\n",
            "4. What benefits did he get from spending time outdoors?\n",
            "5. What did he appreciate most about the day?\n"
          ]
        }
      ],
      "source": [
        "#grammar: past simple\n",
        "#vocabulary: hiking, camping, rock climbing, cycling, kayaking, canoeing, surfing, paddle boarding, skiing, snowboarding, trail running, mountain biking, nature walk, outdoor workout, team sports, adventure sports, physical activity, fresh air, natural environment, open spaces, equipment, safety rules, protective gear, helmet, comfortable clothing, weather conditions, sunny day, rainy weather, cold temperatures, warm temperatures, risk, challenge, endurance, strength, balance, coordination, teamwork, motivation, mental health, physical health, stress reduction, well-being, healthy lifestyle, free time, leisure activities, outdoor experience\n",
        "# Ejercicio: Reading comprehension\n",
        "# =========================\n",
        "# INPUT DEL USUARIO\n",
        "# =========================\n",
        "\n",
        "grammar = input(\"Include the grammar focus (e.g. past simple, modal verbs, conditionals): \")\n",
        "\n",
        "vocabulary_input = input(\n",
        "    \"Enter a list of vocabulary words separated by commas: \"\n",
        ")\n",
        "\n",
        "# Convertimos el vocabulario en una lista limpia\n",
        "vocabulary = [word.strip() for word in vocabulary_input.split(\",\")]\n",
        "\n",
        "# =========================\n",
        "# PROMPT COMPLETO\n",
        "# =========================\n",
        "\n",
        "prompt = f\"\"\"[SYSTEM]\n",
        "You are an English teacher for vocational training students. You create reading comprehension activities adapted to B1 level.\n",
        "\n",
        "[USER]\n",
        "Create a reading comprehension activity.\n",
        "\n",
        "The activity must focus on the following grammar topic:\n",
        "{grammar}\n",
        "\n",
        "Try to naturally include the following vocabulary in the text:\n",
        "{\", \".join(vocabulary)}\n",
        "\n",
        "PART 1 – READING TEXT\n",
        "- Write a short reading text of about 180–220 words.\n",
        "- The text must be appropriate for B1 level.\n",
        "- Use clear language and realistic situations related to everyday life or vocational contexts.\n",
        "- Avoid overly technical vocabulary.\n",
        "- Do not include titles or headings.\n",
        "\n",
        "PART 2 – COMPREHENSION QUESTIONS\n",
        "- Write EXACTLY 5 comprehension questions based on the text.\n",
        "- The questions must test understanding of ideas, reasons, consequences, or implicit information.\n",
        "- Do NOT copy phrases or sentences directly from the text.\n",
        "- Do NOT ask questions that can be answered by matching the same words from the text.\n",
        "- Paraphrase ideas when forming the questions.\n",
        "- Avoid very obvious factual questions (names, dates, places).\n",
        "- Use different question types (why, how, what can be inferred, what is the main idea).\n",
        "- Do NOT include the answers.\n",
        "- Stop writing immediately after the 5th question.\n",
        "\n",
        "Write the reading text first, then the questions.\n",
        "\n",
        "[ASSISTANT]\n",
        "\"\"\"\n",
        "\n",
        "# =========================\n",
        "# INFERENCIA CONTROLADA\n",
        "# =========================\n",
        "# Tokenizamos el prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "prompt_length = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=250,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.1,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# Nos quedamos SOLO con los nuevos tokens (la respuesta)\n",
        "generated_tokens = outputs[0][prompt_length:]\n",
        "\n",
        "result = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GwgLCXLb0xqI",
      "metadata": {
        "id": "GwgLCXLb0xqI"
      },
      "source": [
        "# Evaluación ejercicio Reading Comprehension\n",
        "## Conclusiones sobre el ejercicio de reading comprehension con enfoque gramatical (conditionals)\n",
        "\n",
        "El ejercicio de comprensión lectora generado a partir de un texto contextualizado en actividades al aire libre presenta un nivel lingüístico adecuado para alumnado de B1. El texto es claro, coherente y utiliza un léxico relacionado con actividades físicas y tiempo libre, lo que favorece la motivación y la comprensión global del contenido.\n",
        "\n",
        "En cuanto a la integración de la **gramática (conditionals)**, se observa que el texto proporciona un contexto narrativo válido, pero la presencia de la estructura gramatical no es explícita ni se explota directamente en las preguntas de comprensión. Esto es coherente con un enfoque de lectura cuyo objetivo principal es la comprensión del significado, y no la evaluación directa de la gramática.\n",
        "\n",
        "Respecto a las **preguntas de comprensión**, se ha detectado que la mayoría son de carácter literal. Varias de ellas pueden responderse mediante la localización directa de información en el texto, sin necesidad de realizar inferencias o reformular ideas. Esto limita el desarrollo de estrategias de comprensión más profundas, como la deducción de consecuencias, la interpretación de intenciones o la reflexión personal a partir del contenido leído.\n",
        "\n",
        "Para mejorar el valor didáctico del ejercicio, especialmente en relación con el enfoque gramatical en conditionals, sería recomendable:\n",
        "- formular preguntas que inviten al alumnado a imaginar situaciones alternativas o consecuencias hipotéticas relacionadas con el texto,\n",
        "- promover inferencias basadas en el contenido (por ejemplo, qué ocurriría si las condiciones fueran diferentes),\n",
        "- evitar preguntas que reproduzcan el mismo vocabulario o estructura que aparece de forma explícita en el texto.\n",
        "\n",
        "En conclusión, el ejercicio es válido como práctica de comprensión lectora general, pero puede optimizarse mediante un diseño de preguntas más inferencial y una explotación más clara del enfoque gramatical propuesto. Esto permitiría un mejor equilibrio entre comprensión lectora y reflexión lingüística, alineándose con los objetivos comunicativos del nivel B1.\n",
        "\n",
        "ChatGPT nos muestra la siguiente propuesta de preguntas utilizando el mismo texto: \n",
        "\n",
        "1. What does the text suggest about Mark’s routine before this trip?\n",
        "2. Why do you think Mark enjoyed the day more than a normal workday?\n",
        "3. How did the different activities contribute to Mark’s mood?\n",
        "4. What can be inferred about the importance of nature for Mark?\n",
        "5. In what way did the day help Mark personally?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "RdL9cBZZqWfz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdL9cBZZqWfz",
        "outputId": "61c2f2f1-fa7f-4314-880b-e5f4d1cf7408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the first grammar topic (e.g. present simple): past simple\n",
            "Enter a second grammar topic for contrast (e.g. past continuous): past continuous\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exercise 1:\n",
            "1. Yesterday, I ______ (meet) a client.\n",
            "2. She ______ (work) late yesterday.\n",
            "3. We ______ (finish) the task at noon.\n",
            "4. He ______ (have) a meeting at ten.\n",
            "5. They ______ (start) the project last week.\n",
            "\n",
            "Exercise 2:\n",
            "1. Yesterday, I ______ (meet) a client. (past simple)\n",
            "2. She ______ (work) late yesterday. (past simple)\n",
            "3. We ______ (finish) the task at noon. (past simple)\n",
            "4. He ______ (have) a meeting at ten. (past simple)\n",
            "5. They ______ (start) the project last week. (past simple)\n",
            "6. Yesterday, she ______ (write) the report. (past continuous)\n",
            "7. Last night, he ______ (check) the emails. (past continuous)\n",
            "8. This morning, they ______ (prepare) the documents. (past continuous)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# =========================\n",
        "# INPUT DEL USUARIO\n",
        "# =========================\n",
        "\n",
        "grammar_1 = input(\"Enter the first grammar tense (e.g. present simple): \")\n",
        "grammar_2 = input(\n",
        "    \"Enter a second grammar tense for contrast (e.g. past continuous): \"\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# PROMPT COMPLETO\n",
        "# =========================\n",
        "\n",
        "prompt = f\"\"\"[SYSTEM]\n",
        "You are an English teacher for vocational training students. You create grammar exercises adapted to B1 level.\n",
        "\n",
        "[USER]\n",
        "Create TWO grammar exercises (fill in the gaps).\n",
        "\n",
        "EXERCISE 1 – Single grammar focus\n",
        "- Focus on the following grammar topic: {grammar_1}\n",
        "- Write EXACTLY 5 sentences.\n",
        "- Each sentence must contain ONE gap.\n",
        "- Students must complete the gap using the correct grammatical form.\n",
        "- Do NOT include the answers.\n",
        "\n",
        "EXERCISE 2 – Grammar contrast\n",
        "- Focus on the contrast between these two grammar topics:\n",
        "  {grammar_1} vs {grammar_2}\n",
        "- Write EXACTLY 5 sentences.\n",
        "- Each sentence must contain ONE gap.\n",
        "- Each sentence must clearly require choosing between the two grammar forms.\n",
        "- Use realistic and clear contexts.\n",
        "- Do NOT include the answers.\n",
        "\n",
        "Write Exercise 1 first, then Exercise 2.\n",
        "Stop writing after the last sentence of Exercise 2.\n",
        "\n",
        "[ASSISTANT]\n",
        "\"\"\"\n",
        "\n",
        "# =========================\n",
        "# INFERENCIA CONTROLADA\n",
        "# =========================\n",
        "# Tokenizamos el prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "prompt_length = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=250,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.1,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# Nos quedamos SOLO con los nuevos tokens (la respuesta)\n",
        "generated_tokens = outputs[0][prompt_length:]\n",
        "\n",
        "result = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hTA_WrOt2scc",
      "metadata": {
        "id": "hTA_WrOt2scc"
      },
      "source": [
        "# Valoración ejercicios fill in the gaps (grammar)\n",
        "Los ejercicios de gramática generados mediante el modelo muestran resultados desiguales en función del tipo de tarea planteada y del nivel de control aplicado al prompt.\n",
        "\n",
        "En los ejercicios de **práctica de un solo tiempo verbal** (por ejemplo, *past simple*), el modelo ofrece resultados satisfactorios. Las frases muestran expresiones de tiempo claras, un único hueco por oración y un nivel adecuado para alumnado de B1. Este tipo de ejercicio es sencillo de generar automáticamente, ya que no requiere toma de decisiones complejas por parte del estudiante.\n",
        "\n",
        "Sin embargo, en los ejercicios de **contraste entre dos tiempos verbales** (como *past simple* vs *past continuous*), se ha observado que el modelo tiende inicialmente a producir frases que no evalúan el contraste real, limitándose a la conjugación de un único tiempo o incluso indicando explícitamente qué forma debe usarse. Esto reduce el valor pedagógico del ejercicio y elimina el componente de reflexión gramatical.\n",
        "\n",
        "Para que los ejercicios de contraste sean válidos desde un punto de vista didáctico, es necesario que:\n",
        "- cada frase incluya un contexto que obligue a elegir entre los dos tiempos verbales,\n",
        "- se presenten dos acciones relacionadas (acción en progreso e interrupción, acciones simultáneas, etc.),\n",
        "- no se indique explícitamente qué tiempo verbal debe emplearse.\n",
        "\n",
        "Tras ajustar el diseño del ejercicio y el prompt, los resultados mejoran significativamente, generando frases más cercanas a las que aparecen en pruebas de evaluación reales de nivel B1.\n",
        "\n",
        "En conclusión, el uso de modelos de lenguaje para la generación de ejercicios gramaticales es especialmente eficaz cuando:\n",
        "- el tipo de ejercicio está bien definido,\n",
        "- el grado de libertad del modelo es limitado,\n",
        "- y se controla cuidadosamente el formato y la intención evaluativa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "532ac2af",
      "metadata": {},
      "source": [
        "## Conclusiones\n",
        "\n",
        "\n",
        "\n",
        "Los resultados de ejercicios que hemos obtenido con el modelo Mistral 7b instruct fine-tuneado con un dataset artificial han sido, si bien interesantes, aún no satisfactorios para darle un uso profesional del cual profesores y alumnos puedan sacar partido para practicar, generar modelos para exámenes, etc. \n",
        "\n",
        "Uno de los principales problemas detectados es la dificultad del modelo para interpretar correctamente finalidad de la tarea cuando la tarea exige un control muy preciso del tipo de respuesta esperada. En estos casos, el modelo tiende a priorizar la corrección gramatical general frente a la utilidad específica del ejercicio, creando un ejercicio inacabado, incompleto, o que requiere muy poco esfuerzo por parte del alumno\n",
        "\n",
        "Aun así, los resultados pueden considerarse prometedores. El experimento demuestra que, mediante un prompting habilidoso y un proceso de entrenamiento adecuado, los modelos de lenguaje pueden generar materiales educativos funcionales y coherentes. El uso de Mistral 7B Instruct resulta especialmente interesante por tratarse de un modelo gratuito, lo que refuerza su atractivo en entornos educativos con recursos limitados, a pesar de las limitaciones observadas.\n",
        "\n",
        "De cara a trabajos futuros, se plantean varias líneas de mejora y exploración:\n",
        "- Probar tareas menos dependientes de estructuras cerradas, como actividades de *speaking* o *writing*, donde el modelo puede explotar mejor su capacidad generativa.\n",
        "- Experimentar con modelos de mayor tamaño y número de parámetros, manteniendo los mismos criterios de fine-tuning, para analizar si se obtiene una mejora significativa en la precisión y control de las respuestas.\n",
        "- Comparar el rendimiento del modelo entrenado con otros modelos más avanzados, como Gemini, con el fin de evaluar hasta qué punto el tamaño y la arquitectura influyen en la generación de materiales didácticos de calidad.\n",
        "\n",
        "En conclusión, aunque el modelo utilizado aún no ofrece resultados plenamente satisfactorios para todas las tipologías de ejercicios, el trabajo realizado evidencia el potencial de los modelos de lenguaje como herramientas de apoyo docente, siempre que su uso vaya acompañado de un diseño pedagógico sólido y de una supervisión humana constante.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mlops-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "164e44f7c7464e718507925e26ea898f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "173f42cc80a84d14843834c43e8ae617": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ed38659da254ea595bf37dd5847a33b",
              "IPY_MODEL_4bddf8fccddf45f991220d8e14f94031",
              "IPY_MODEL_4ed5c22356b74ae88c2f95d10d5c83f9"
            ],
            "layout": "IPY_MODEL_f5767c46597e42a18793c9e61e01081d"
          }
        },
        "21ccbc61e59d40cfbcbb911db51e5869": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "239f2a5afece45bbb125145388bec625": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2506de45fe3648d0a41d4d92d62c418a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2591d0bf349a46ee8544c2774651f4b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bddf8fccddf45f991220d8e14f94031": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77bed94f35b14359bf5cf747cea56351",
            "max": 60,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50dabfb5338546b991340ee04b96e59b",
            "value": 60
          }
        },
        "4ed5c22356b74ae88c2f95d10d5c83f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21ccbc61e59d40cfbcbb911db51e5869",
            "placeholder": "​",
            "style": "IPY_MODEL_2591d0bf349a46ee8544c2774651f4b7",
            "value": " 60/60 [00:00&lt;00:00, 902.97 examples/s]"
          }
        },
        "50dabfb5338546b991340ee04b96e59b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "532c2bc7f6d54a6280a7d89760e90cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d285bd18989c494fa31de83cabb93ef2",
            "placeholder": "​",
            "style": "IPY_MODEL_9c2d4477aa3e484aa2a1983ebfbbc494",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "62d262e494124811b57797677acba505": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "656d0cbe27b54166a7b19b84c2dd909d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_532c2bc7f6d54a6280a7d89760e90cbe",
              "IPY_MODEL_f723d01802104a5daeabb1a29cf8a705",
              "IPY_MODEL_9782e0af68934df984308dd9cf4b87a6"
            ],
            "layout": "IPY_MODEL_164e44f7c7464e718507925e26ea898f"
          }
        },
        "6ed38659da254ea595bf37dd5847a33b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_836a07a301524c709a9e5b547c7396e9",
            "placeholder": "​",
            "style": "IPY_MODEL_f2c3004756dc423289f89f002336cbd6",
            "value": "Map: 100%"
          }
        },
        "77bed94f35b14359bf5cf747cea56351": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "836a07a301524c709a9e5b547c7396e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9782e0af68934df984308dd9cf4b87a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2506de45fe3648d0a41d4d92d62c418a",
            "placeholder": "​",
            "style": "IPY_MODEL_239f2a5afece45bbb125145388bec625",
            "value": " 3/3 [01:16&lt;00:00, 25.93s/it]"
          }
        },
        "9c2d4477aa3e484aa2a1983ebfbbc494": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be8336e53ff64b6b99165323c0ae4e38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d285bd18989c494fa31de83cabb93ef2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2c3004756dc423289f89f002336cbd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5767c46597e42a18793c9e61e01081d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f723d01802104a5daeabb1a29cf8a705": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be8336e53ff64b6b99165323c0ae4e38",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62d262e494124811b57797677acba505",
            "value": 3
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
